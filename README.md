# ğŸ” RAG Agent

A production-quality **Retrieval-Augmented Generation** agent that lets you upload documents and ask questions with source-cited answers. Built with LangChain, ChromaDB, and Streamlit.

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3-green.svg)](https://python.langchain.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## âœ¨ Features

- **ğŸ“„ Multi-format document ingestion** â€” PDF, TXT, and Markdown
- **ğŸ§  Intelligent chunking** â€” Recursive text splitting with configurable size and overlap
- **ğŸ” Semantic search** â€” Vector similarity retrieval via ChromaDB or FAISS
- **ğŸ’¬ Conversational UI** â€” Clean Streamlit chat interface with message history
- **ğŸ“š Source citations** â€” Every answer references the exact documents and pages used
- **ğŸ”Œ Pluggable LLM** â€” Switch between OpenAI, Anthropic, or Ollama with one env var
- **ğŸ³ Dockerized** â€” One command to build and run

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit   â”‚â”€â”€â”€â”€â–¶â”‚   Ingestion  â”‚â”€â”€â”€â”€â–¶â”‚   Vector Store  â”‚
â”‚     UI       â”‚     â”‚   Pipeline   â”‚     â”‚  (ChromaDB /    â”‚
â”‚              â”‚     â”‚              â”‚     â”‚   FAISS)        â”‚
â”‚  Upload &    â”‚     â”‚  Load â”€â”€â–¶    â”‚     â”‚                 â”‚
â”‚  Chat        â”‚     â”‚  Split â”€â”€â–¶   â”‚     â”‚  Embeddings     â”‚
â”‚              â”‚     â”‚  Embed â”€â”€â–¶   â”‚     â”‚  stored here    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Index       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
       â”‚                                           â”‚
       â”‚  Question                                 â”‚  Retrieval
       â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Chain  â”‚â”€â”€â”€â”€â–¶â”‚     LLM      â”‚â”€â”€â”€â”€â–¶â”‚    Response +   â”‚
â”‚              â”‚     â”‚  (OpenAI /   â”‚     â”‚    Source        â”‚
â”‚  Prompt +    â”‚     â”‚  Anthropic / â”‚     â”‚    Citations     â”‚
â”‚  Context     â”‚     â”‚  Ollama)     â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- An LLM API key (OpenAI, Anthropic, or local Ollama)
- **No API key needed for embeddings** â€” uses local HuggingFace model by default

### 1. Clone & Install

```bash
git clone https://github.com/bludragon-ai/rag-agent.git
cd rag-agent

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

### 2. Configure

```bash
cp .env.example .env
# Edit .env and add your API key
```

### 3. Run

```bash
make run
# or: streamlit run src/ui/app.py
```

Open [http://localhost:8501](http://localhost:8501) in your browser.

### 4. Try It

1. Upload the sample documents from `docs/sample/`
2. Click **Index Documents**
3. Ask: *"What is RAG and how does it work?"*

## ğŸ³ Docker

```bash
# Build and run
cp .env.example .env  # configure your API key
docker compose up --build

# Access at http://localhost:8501
```

## ğŸ”§ Configuration

All settings are controlled via environment variables (see [`.env.example`](.env.example)):

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | `openai` | LLM backend: `openai`, `anthropic`, `ollama` |
| `OPENAI_API_KEY` | â€” | Your OpenAI API key |
| `OPENAI_MODEL` | `gpt-4o-mini` | OpenAI model to use |
| `EMBEDDING_PROVIDER` | `huggingface` | Embedding backend: `huggingface` (local) or `openai` |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Embedding model name |
| `VECTOR_STORE` | `chroma` | Vector store: `chroma` or `faiss` |
| `CHUNK_SIZE` | `1000` | Document chunk size (characters) |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `RETRIEVAL_TOP_K` | `4` | Number of chunks retrieved per query |

### Using with Anthropic (no OpenAI key needed)

```bash
# Set in .env:
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-your-key-here
# Embeddings use local HuggingFace by default â€” no extra API key needed!
```

### Using with Ollama (fully local, no API key needed)

```bash
# Install Ollama: https://ollama.ai
ollama pull llama3

# Set in .env:
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3
# Embeddings use local HuggingFace by default â€” fully local, no API keys!
```

## ğŸ“ Project Structure

```
rag-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # Centralized settings (pydantic-settings)
â”‚   â”‚   â”œâ”€â”€ llm.py           # LLM factory (OpenAI / Anthropic / Ollama)
â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Embedding model factory
â”‚   â”‚   â”œâ”€â”€ vectorstore.py   # Vector store factory (Chroma / FAISS)
â”‚   â”‚   â”œâ”€â”€ ingest.py        # Document loading & chunking pipeline
â”‚   â”‚   â””â”€â”€ chain.py         # RAG chain with LCEL
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ app.py           # Streamlit web interface
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logging.py       # Logging configuration
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ docs/sample/             # Sample documents for demo
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â””â”€â”€ .env.example
```

## ğŸ§ª Testing

```bash
make test
# or: pytest tests/ -v
```

## ğŸ›£ï¸ Roadmap

- [ ] Conversation memory (multi-turn context)
- [ ] Hybrid search (keyword + semantic)
- [ ] Document management UI (view/delete indexed docs)
- [ ] Streaming responses
- [ ] Authentication & multi-user support
- [ ] API endpoint (FastAPI) alongside the UI

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE) for details.

---

Built by [bludragon-ai](https://github.com/bludragon-ai)
